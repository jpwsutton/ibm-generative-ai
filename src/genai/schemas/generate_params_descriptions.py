# flake8: noqa
# fmt: off
# Schema Descriptions class for: GenerateParams
#     Generated by: James Sutton: james.sutton@uk.ibm.com
#     Generated on: 2023-08-03T15:48:49.401229
#     Generated from Schema at: https://bam-api.res.ibm.com/v1/models/full: schemas_generate
class GenerateParamsDescriptions:
    TOP_K = "Set the number of highest probability vocabulary tokens to keep for top-k-filtering. Lower values make it less likely the model will go off topic."
    TOP_P = "If < 1.0, only the smallest set of most probable tokens with probabilities that add up to `top_p` or higher are used."
    STREAM = "Enables to stream partial progress as server-sent events."
    TYPICAL_P = "Local typicality measures how similar the conditional probability of predicting a target token next is to the expected conditional probability of predicting a random token next, given the partial text already generated. If set to float < 1, the smallest set of the most locally typical tokens with probabilities that add up to typical_p or higher are kept for generation. 1.00 means a neutral value."
    BEAM_WIDTH = "At each step, or token, the algorithm keeps track of the n (off=1, 2, or 3) most probable sequences (beams) and selects the one with the highest probability. This continues until the stop sequence is met."
    TIME_LIMIT = "Time limit in milliseconds - if not completed within this time, generation will stop. The text generated so far will be returned along with the `TIME_LIMIT` stop reason."
    RANDOM_SEED = "Controls the random sampling of the generated tokens when sampling is enabled. Setting the random seed to a the same number for each generation ensures experimental repeatability."
    TEMPERATURE = "Control the creativity of generated text. Higher values will lead to more randomly generated outputs."
    LENGTH_PENALTY = "Can be used to exponentially increase the likelihood of the text generation terminating once a specified number of tokens have been generated."
    MAX_NEW_TOKENS = "Define the maximum number of tokens to generate."
    MIN_NEW_TOKENS = "If stop sequences are given, they are ignored until minimum tokens are generated."
    STOP_SEQUENCES = "Stop sequences are one or more strings which will cause the text generation to stop if/when they are produced as part of the output. Stop sequences encountered prior to the minimum number of tokens being generated will be ignored."
    DECODING_METHOD = "Decoding method used for generation."
    REPETITION_PENALTY = "The parameter for repetition penalty. 1.00 means no penalty."
    TRUNCATE_INPUT_TOKENS = "Truncate to this many input tokens. Can be used to avoid requests failing due to input being longer than configured limits. Zero means don't truncate."
